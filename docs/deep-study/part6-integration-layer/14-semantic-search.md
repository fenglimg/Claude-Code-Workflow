# Chapter 14: è¯­ä¹‰è¿½è¸ªè€… â€” CodexLens æœç´¢ç³»ç»Ÿ

> **ç”Ÿå‘½å‘¨æœŸé˜¶æ®µ**: æŸ¥è¯¢è§£æ â†’ å¹¶è¡Œæœç´¢ â†’ RRF èåˆ â†’ èšç±»å»é‡
> **ç‰ˆæœ¬è¿½è¸ª**: `docs/.audit-manifest.json`
> **é˜…è¯»æ—¶é—´**: 55-70 åˆ†é’Ÿ

---

## åºå¹•ï¼šOOM å¹½çµçš„è¯­ä¹‰æŒ‡çº¹

åœ¨ CCW è¿™åº§æ•°å­—åŸå¸‚ä¸­ï¼ŒOOM å¹½çµç•™ä¸‹çš„ç—•è¿¹ä¸ä»…æ˜¯ä»£ç å’Œæ—¥å¿—ï¼Œè¿˜æœ‰**è¯­ä¹‰æŒ‡çº¹**â€”â€”é‚£äº›éšè—åœ¨ä»£ç å«ä¹‰ä¸­çš„çº¿ç´¢ï¼š

| æœç´¢æ–¹å¼ | åŸç† | æ“…é•¿æ•è· |
|----------|------|----------|
| **FTS (å…¨æ–‡æœç´¢)** | å…³é”®è¯åŒ¹é… | ç²¾ç¡®å‡½æ•°åã€å˜é‡å |
| **SPLADE (ç¨€ç–ç¥ç»)** | ç¨€ç–è¯æ±‡æ‰©å±• | åŒä¹‰è¯ã€ç›¸å…³æ¦‚å¿µ |
| **Vector (å‘é‡æœç´¢)** | è¯­ä¹‰ç›¸ä¼¼åº¦ | è‡ªç„¶è¯­è¨€æè¿°ã€ä»£ç æ„å›¾ |
| **LSP Graph (å›¾æ‰©å±•)** | ä»£ç å¼•ç”¨å…³ç³» | è°ƒç”¨é“¾ã€ä¾èµ–å…³ç³» |

CodexLens çš„ `HybridSearchEngine` æ˜¯è¿½è¸ªè¿™äº›è¯­ä¹‰æŒ‡çº¹çš„æ ¸å¿ƒå¼•æ“ã€‚

---

## è‹æ ¼æ‹‰åº•å¼æ€è€ƒ

> **æ¶æ„ç”Ÿæ­»æˆ˜ 14**: ç”¨æˆ·æœç´¢ "å†…å­˜æ³„æ¼æ€ä¹ˆä¿®å¤"ï¼ŒæœŸæœ›æ‰¾åˆ°ï¼š
> - ç›´æ¥åŒ…å« "å†…å­˜æ³„æ¼" çš„ä»£ç æ³¨é‡Š
> - è°ƒç”¨ `free()`ã€`delete`ã€`GC.collect()` çš„æ¸…ç†é€»è¾‘
> - ä¸å†…å­˜ç®¡ç†ç›¸å…³çš„å·¥å…·ç±»
>
> å•ä¸€æœç´¢æ–¹å¼æ— æ³•æ»¡è¶³æ‰€æœ‰éœ€æ±‚ã€‚ä½ ä¼šå¦‚ä½•è®¾è®¡ä¸€ä¸ª**æ··åˆæœç´¢ç³»ç»Ÿ**ï¼Ÿæ¯ç§é€‰æ‹©éƒ½ä¼šå½±å“å¬å›ç‡å’Œç²¾ç¡®åº¦çš„å¹³è¡¡ã€‚

---

## ç¬¬ä¸€å¹•ï¼šå¤±æ§çš„è¾¹ç¼˜ (Out of Control)

### å•ä¸€æœç´¢çš„å±€é™æ€§

**åœºæ™¯ä¸€ï¼šFTS çš„ç›²åŒº**

```sql
-- FTS æŸ¥è¯¢
SELECT * FROM code WHERE content MATCH 'memory leak';

-- é—®é¢˜ï¼šæ‰¾ä¸åˆ°ä»¥ä¸‹ä»£ç 
def cleanup_resources():  # æ²¡æœ‰ç›´æ¥å‡ºç° "memory leak"
    """é‡Šæ”¾æ‰€æœ‰å ç”¨çš„å†…å­˜èµ„æº"""
    for item in cache:
        del item
```

**åœºæ™¯äºŒï¼šå‘é‡æœç´¢çš„æ­§ä¹‰**

```python
# å‘é‡æœç´¢ "å†…å­˜æ³„æ¼"
# å¯èƒ½è¿”å›ï¼š
def memory_game():
    """ä¸€ä¸ªè®°å¿†å¡ç‰‡æ¸¸æˆ"""  # è¯­ä¹‰ç›¸ä¼¼ï¼Œä½†æ— å…³
    pass
```

**åœºæ™¯ä¸‰ï¼šç¼ºå°‘ä¸Šä¸‹æ–‡**

```python
# FTS æ‰¾åˆ°äº†è¿™ä¸ªå‡½æ•°
def process_data(data):
    # ä½†ä¸çŸ¥é“è°è°ƒç”¨äº†å®ƒï¼Œä¹Ÿä¸çŸ¥é“å®ƒè°ƒç”¨äº†è°
    return transform(data)
```

**é—®é¢˜çš„æœ¬è´¨**ï¼š

æœç´¢ç³»ç»Ÿé¢ä¸´çš„**ä¸‰è§’å›°å¢ƒ**ï¼š

```
          å¬å›ç‡ (Recall)
              /\
             /  \
            /    \
ç²¾ç¡®åº¦ â†-------â†’ è¦†ç›–èŒƒå›´ (Coverage)
       (Precision)
```

---

## ç¬¬äºŒå¹•ï¼šæ€ç»´è„‰ç»œ (The Neural Link)

### æ··åˆæœç´¢æ¶æ„

```mermaid
graph TB
    subgraph "è¾“å…¥å±‚"
        A[ç”¨æˆ·æŸ¥è¯¢]
        B[QueryIntent æ£€æµ‹]
    end

    subgraph "å¹¶è¡Œæœç´¢å±‚ â€” HybridSearchEngine"
        C[Exact FTS]
        D[Fuzzy FTS]
        E[SPLADE Sparse]
        F[Vector Dense]
        G[LSP Graph]
    end

    subgraph "èåˆå±‚ â€” RRF"
        H[Reciprocal Rank Fusion]
        I[Symbol Boost]
        J[Category Filter]
    end

    subgraph "èšç±»å±‚"
        K[HDBSCAN Clustering]
        L[Representative Selection]
    end

    A --> B
    B --> C
    B --> D
    B --> E
    B --> F
    B --> G
    
    C --> H
    D --> H
    E --> H
    F --> H
    G --> H
    
    H --> I
    I --> J
    J --> K
    K --> L
```

### Embedder æ¨¡å‹é€‰æ‹©æµç¨‹

```mermaid
flowchart TD
    A[åˆå§‹åŒ– Embedder] --> B{æ£€æŸ¥ GPU å¯ç”¨æ€§}
    B -->|GPU å¯ç”¨| C[é€‰æ‹© GPU Provider]
    B -->|ä»… CPU| D[CPUExecutionProvider]
    
    C --> E{æ“ä½œç³»ç»Ÿ}
    E -->|Windows| F[CUDA/DirectML]
    E -->|Linux| G[CUDA/ROCm]
    E -->|macOS| H[CoreML]
    
    F --> I[åŠ è½½ ONNX æ¨¡å‹]
    G --> I
    H --> I
    D --> I
    
    I --> J[ç”Ÿæˆå‘é‡åµŒå…¥]
```

### HDBSCAN èšç±»æµç¨‹

```mermaid
flowchart LR
    A[æœç´¢ç»“æœ] --> B[æå– Embeddings]
    B --> C[HDBSCAN èšç±»]
    
    C --> D[æ­£å¸¸èšç±» label >= 0]
    C --> E[å™ªå£°ç‚¹ label = -1]
    
    D --> F[é€‰æ‹©ä»£è¡¨ç»“æœ]
    E --> G[ä½œä¸ºå•ç‹¬èšç±»]
    
    F --> H[æŒ‰åˆ†æ•°æ’åº]
    G --> H
    
    H --> I[è¿”å›å»é‡ç»“æœ]
```

### æŸ¥è¯¢æ„å›¾ä¸æƒé‡è°ƒæ•´

```mermaid
flowchart TD
    A[ç”¨æˆ·æŸ¥è¯¢] --> B[detect_query_intent]
    
    B --> C{æŸ¥è¯¢æ„å›¾}
    C -->|KEYWORD| D[ä»£ç æ„å›¾]
    C -->|SEMANTIC| E[è¯­ä¹‰æ„å›¾]
    C -->|MIXED| F[æ··åˆæ„å›¾]
    
    D --> G[splade: 0.6, vector: 0.4]
    E --> H[splade: 0.3, vector: 0.7]
    F --> I[é»˜è®¤æƒé‡]
    
    G --> J[RRF èåˆ]
    H --> J
    I --> J
```

### Embedder æ¨¡å‹é…ç½®

CodexLens æ”¯æŒå¤šç§åµŒå…¥æ¨¡å‹ï¼Œæ¯ç§æœ‰ä¸åŒçš„ç‰¹ç‚¹å’Œé€‚ç”¨åœºæ™¯ï¼š

```python
# codex-lens/src/codexlens/semantic/embedder.py

class Embedder(BaseEmbedder):
    """Generate embeddings for code chunks using fastembed (ONNX-based).

    Supported Model Profiles:
    - fast: BAAI/bge-small-en-v1.5 (384 dim) - Fast, lightweight, English-optimized
    - code: jinaai/jina-embeddings-v2-base-code (768 dim) - Code-optimized
    - multilingual: intfloat/multilingual-e5-large (1024 dim) - Multilingual + code
    - balanced: mixedbread-ai/mxbai-embed-large-v1 (1024 dim) - High accuracy
    """

    MODELS = {
        "fast": "BAAI/bge-small-en-v1.5",           # 384 dim
        "code": "jinaai/jina-embeddings-v2-base-code",  # 768 dim
        "multilingual": "intfloat/multilingual-e5-large",  # 1024 dim
        "balanced": "mixedbread-ai/mxbai-embed-large-v1",  # 1024 dim
    }

    MODEL_DIMS = {
        "BAAI/bge-small-en-v1.5": 384,
        "jinaai/jina-embeddings-v2-base-code": 768,
        "intfloat/multilingual-e5-large": 1024,
        "mixedbread-ai/mxbai-embed-large-v1": 1024,
    }
```

**æ¨¡å‹é€‰æ‹©ç­–ç•¥**ï¼š

| åœºæ™¯ | æ¨èæ¨¡å‹ | åŸå›  |
|------|----------|------|
| å®æ—¶æœç´¢ | `fast` | 384 ç»´ï¼Œå»¶è¿Ÿæœ€ä½ |
| ä»£ç æœç´¢ | `code` | 768 ç»´ï¼Œä»£ç è¯­ä¹‰ç†è§£æœ€å¼º |
| å¤šè¯­è¨€é¡¹ç›® | `multilingual` | 1024 ç»´ï¼Œæ”¯æŒä¸­è‹±æ—¥éŸ©ç­‰ |
| é«˜ç²¾åº¦éœ€æ±‚ | `balanced` | 1024 ç»´ï¼Œç»¼åˆè¡¨ç°æœ€ä½³ |

### GPU åŠ é€Ÿæ”¯æŒ

```python
# codex-lens/src/codexlens/semantic/gpu_support.py

def get_optimal_providers(use_gpu: bool = True, with_device_options: bool = False) -> List[str]:
    """Get optimal ONNX execution providers based on available hardware.
    
    Provider Priority:
    1. CUDA (NVIDIA GPU)
    2. TensorRT (NVIDIA GPU, optimized)
    3. DirectML (Windows GPU)
    4. ROCm (AMD GPU)
    5. CoreML (Apple Silicon)
    6. CPU (fallback)
    """
    if not use_gpu:
        return ['CPUExecutionProvider']
    
    providers = []
    
    # æ£€æµ‹å¯ç”¨çš„ GPU æä¾›è€…
    available = ort.get_available_providers()
    
    if 'CUDAExecutionProvider' in available:
        providers.append('CUDAExecutionProvider')
    if 'TensorrtExecutionProvider' in available:
        providers.append('TensorrtExecutionProvider')
    if 'DmlExecutionProvider' in available:  # Windows DirectML
        providers.append('DmlExecutionProvider')
    if 'ROCMExecutionProvider' in available:  # AMD ROCm
        providers.append('ROCMExecutionProvider')
    if 'CoreMLExecutionProvider' in available:  # Apple Silicon
        providers.append('CoreMLExecutionProvider')
    
    # æ€»æ˜¯æ·»åŠ  CPU ä½œä¸ºåå¤‡
    providers.append('CPUExecutionProvider')
    
    return providers
```

---

## ç¬¬ä¸‰å¹•ï¼šç¤¾äº¤ç½‘ç»œ (The Social Network)

### HybridSearchEngine â€” å¹¶è¡Œæœç´¢åè°ƒå™¨

```python
# codex-lens/src/codexlens/search/hybrid_search.py

class HybridSearchEngine:
    """Hybrid search engine with parallel execution and RRF fusion.

    Orchestrates searches across exact FTS, fuzzy FTS, and optional vector backends,
    executing them in parallel and fusing results via Reciprocal Rank Fusion.
    """

    def __init__(
        self,
        weights: Optional[Dict[str, float]] = None,
        config: Optional[Config] = None,
        embedder: Any = None,
    ):
        self.weights = weights or DEFAULT_WEIGHTS.copy()
        self._config = config
        self.embedder = embedder

    def search(
        self,
        index_path: Path,
        query: str,
        limit: int = 20,
        enable_fuzzy: bool = True,
        enable_vector: bool = False,
        pure_vector: bool = False,
        enable_splade: bool = False,
        enable_lsp_graph: bool = False,
    ) -> List[SearchResult]:
        """Execute hybrid search with parallel retrieval and RRF fusion."""
        
        # 1. æ£€æµ‹æŸ¥è¯¢æ„å›¾
        query_intent = detect_query_intent(query)
        
        # 2. ç¡®å®šä½¿ç”¨å“ªäº›åç«¯
        backends = self._determine_backends(
            enable_fuzzy, enable_vector, pure_vector, enable_splade, enable_lsp_graph
        )
        
        # 3. å¹¶è¡Œæ‰§è¡Œæœç´¢
        results_map = self._search_parallel(index_path, query, backends, limit)
        
        # 4. RRF èåˆ
        adaptive_weights = get_rrf_weights(query, self._get_active_weights(results_map))
        fused_results = reciprocal_rank_fusion(results_map, adaptive_weights, k=60)
        
        # 5. Symbol Boost
        fused_results = apply_symbol_boost(fused_results, boost_factor=1.5)
        
        # 6. åˆ†ç±»è¿‡æ»¤
        fused_results = filter_results_by_category(fused_results, query_intent)
        
        return fused_results[:limit]
```

### å¹¶è¡Œæœç´¢å®ç°

```python
def _search_parallel(
    self,
    index_path: Path,
    query: str,
    backends: Dict[str, bool],
    limit: int,
) -> Dict[str, List[SearchResult]]:
    """Execute parallel searches across enabled backends."""
    results_map: Dict[str, List[SearchResult]] = {}
    
    # ä½¿ç”¨ ThreadPoolExecutor è¿›è¡Œå¹¶è¡Œ I/O å¯†é›†æœç´¢
    with ThreadPoolExecutor(max_workers=len(backends)) as executor:
        future_to_source = {}
        
        if backends.get("exact"):
            future = executor.submit(self._search_exact, index_path, query, limit)
            future_to_source[future] = "exact"
        
        if backends.get("fuzzy"):
            future = executor.submit(self._search_fuzzy, index_path, query, limit)
            future_to_source[future] = "fuzzy"
        
        if backends.get("vector"):
            future = executor.submit(self._search_vector, index_path, query, limit, category)
            future_to_source[future] = "vector"
        
        if backends.get("splade"):
            future = executor.submit(self._search_splade, index_path, query, limit)
            future_to_source[future] = "splade"
        
        if backends.get("lsp_graph"):
            future = executor.submit(self._search_lsp_graph, index_path, query, limit)
            future_to_source[future] = "lsp_graph"
        
        # æ”¶é›†ç»“æœï¼Œå¸¦è¶…æ—¶ä¿æŠ¤
        for future in as_completed(future_to_source, timeout=30.0):
            source = future_to_source[future]
            try:
                results = future.result(timeout=10.0)
                results_map[source] = tag_search_source(results, source)
            except Exception as exc:
                self.logger.error("Search failed for %s: %s", source, exc)
                results_map[source] = []
    
    return results_map
```

### RRF èåˆç®—æ³•

```python
# codex-lens/src/codexlens/search/ranking.py

def reciprocal_rank_fusion(
    results_map: Dict[str, List[SearchResult]],
    weights: Dict[str, float] = None,
    k: int = 60,
) -> List[SearchResult]:
    """Combine search results from multiple sources using Reciprocal Rank Fusion.

    RRF formula: score(d) = Î£ weight_source / (k + rank_source(d))
    """
    if not results_map:
        return []

    # é»˜è®¤ç­‰æƒé‡
    if weights is None:
        weights = {source: 1.0 / len(results_map) for source in results_map}

    # æ„å»ºç»Ÿä¸€ç»“æœé›†
    path_to_result: Dict[str, SearchResult] = {}
    path_to_fusion_score: Dict[str, float] = {}
    path_to_source_ranks: Dict[str, Dict[str, int]] = {}

    for source_name, results in results_map.items():
        weight = weights.get(source_name, 0.0)
        if weight == 0:
            continue

        for rank, result in enumerate(results, start=1):
            path = result.path
            rrf_contribution = weight / (k + rank)

            if path not in path_to_fusion_score:
                path_to_fusion_score[path] = 0.0
                path_to_result[path] = result
                path_to_source_ranks[path] = {}

            path_to_fusion_score[path] += rrf_contribution
            path_to_source_ranks[path][source_name] = rank

    # åˆ›å»ºèåˆç»“æœ
    fused_results = []
    for path, base_result in path_to_result.items():
        fusion_score = path_to_fusion_score[path]

        fused_result = SearchResult(
            path=base_result.path,
            score=fusion_score,
            excerpt=base_result.excerpt,
            content=base_result.content,
            metadata={
                **base_result.metadata,
                "fusion_method": "rrf",
                "fusion_score": fusion_score,
                "source_ranks": path_to_source_ranks[path],
            },
        )
        fused_results.append(fused_result)

    # æŒ‰èåˆåˆ†æ•°é™åºæ’åˆ—
    fused_results.sort(key=lambda r: r.score, reverse=True)

    return fused_results
```

---

## ç¬¬å››å¹•ï¼šé€ ç‰©ä¸»çš„ç§è¯­ (The Creator's Secret)

### ç§˜å¯†ä¸€ï¼šå‘é‡åŒ– â€” ä»£ç å¦‚ä½•å˜æˆæ•°å­—

**å‘é‡åµŒå…¥çš„æ•°å­¦åŸç†**ï¼š

ç»™å®šä¸¤ä¸ªå‘é‡ $\mathbf{A}$ å’Œ $\mathbf{B}$ï¼Œä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—å…¬å¼ï¼š

$$\text{similarity} = \cos(\theta) = \frac{\mathbf{A} \cdot \mathbf{B}}{\|\mathbf{A}\| \|\mathbf{B}\|} = \frac{\sum_{i=1}^{n} A_i B_i}{\sqrt{\sum_{i=1}^{n} A_i^2} \sqrt{\sum_{i=1}^{n} B_i^2}}$$

**ä»£ç ç¤ºä¾‹**ï¼š

```python
def cosine_similarity(vec_a: List[float], vec_b: List[float]) -> float:
    n = min(len(vec_a), len(vec_b))
    if n == 0:
        return 0.0
    
    dot = 0.0
    norm_a = 0.0
    norm_b = 0.0
    
    for i in range(n):
        a = float(vec_a[i])
        b = float(vec_b[i])
        dot += a * b
        norm_a += a * a
        norm_b += b * b
    
    if norm_a <= 0.0 or norm_b <= 0.0:
        return 0.0
    
    return dot / (math.sqrt(norm_a) * math.sqrt(norm_b))
```

### ç§˜å¯†äºŒï¼šSPLADE â€” ç¨€ç–è¯æ±‡æ‰©å±•

**SPLADE æ¿€æ´»å‡½æ•°**ï¼š

$$\text{SPLADE}(x) = \log(1 + \text{ReLU}(W_x)) \odot M$$

å…¶ä¸­ï¼š
- $W_x$ æ˜¯ MLM (Masked Language Model) çš„ logits è¾“å‡º
- $M$ æ˜¯æ³¨æ„åŠ›æ©ç 
- $\odot$ æ˜¯é€å…ƒç´ ä¹˜æ³•

```python
# codex-lens/src/codexlens/semantic/splade_encoder.py

@staticmethod
def _splade_activation(logits: Any, attention_mask: Any) -> Any:
    """Apply SPLADE activation function to model outputs.

    Formula: log(1 + ReLU(logits)) * attention_mask
    """
    import numpy as np

    # ReLU æ¿€æ´»
    relu_logits = np.maximum(0, logits)

    # Log(1 + x) å˜æ¢
    log_relu = np.log1p(relu_logits)

    # åº”ç”¨æ³¨æ„åŠ›æ©ç 
    mask_expanded = np.expand_dims(attention_mask, axis=-1)
    splade_repr = log_relu * mask_expanded

    return splade_repr

@staticmethod
def _max_pooling(splade_repr: Any) -> Any:
    """Max pooling over sequence length dimension."""
    import numpy as np
    return np.max(splade_repr, axis=1)
```

**SPLADE vs BM25**ï¼š

| ç‰¹æ€§ | BM25 | SPLADE |
|------|------|--------|
| è¯æ±‡åŒ¹é… | ç²¾ç¡® | æ‰©å±•ï¼ˆåŒä¹‰è¯ã€ç›¸å…³è¯ï¼‰ |
| è®¡ç®—å¼€é”€ | ä½ | ä¸­ï¼ˆéœ€è¦ç¥ç»ç½‘ç»œï¼‰ |
| å¯è§£é‡Šæ€§ | é«˜ | ä¸­ï¼ˆå¯æŸ¥çœ‹æ‰©å±•è¯æƒé‡ï¼‰ |
| å¤šè¯­è¨€ | å·® | å¥½ï¼ˆåŸºäºé¢„è®­ç»ƒæ¨¡å‹ï¼‰ |

### ç§˜å¯†ä¸‰ï¼šHDBSCAN â€” å¯†åº¦èšç±»

**ä¸ºä»€ä¹ˆé€‰æ‹© HDBSCAN è€Œé K-Means**ï¼š

| ç‰¹æ€§ | K-Means | HDBSCAN |
|------|---------|---------|
| èšç±»æ•°é‡ | éœ€è¦é¢„å…ˆæŒ‡å®š | è‡ªåŠ¨ç¡®å®š |
| å½¢çŠ¶ | åªèƒ½è¯†åˆ«çƒå½¢ | ä»»æ„å½¢çŠ¶ |
| å™ªå£°å¤„ç† | æ‰€æœ‰ç‚¹å¿…é¡»å±äºæŸç±» | è‡ªåŠ¨è¯†åˆ«å™ªå£°ç‚¹ |
| å‚æ•°æ•æ„Ÿåº¦ | é«˜ | ä½ |

```python
# codex-lens/src/codexlens/search/clustering/hdbscan_strategy.py

class HDBSCANStrategy(BaseClusteringStrategy):
    """HDBSCAN-based clustering strategy.

    HDBSCAN is preferred over DBSCAN because it:
    - Automatically determines the number of clusters
    - Handles varying density clusters well
    - Identifies noise points (outliers) effectively
    """

    def cluster(
        self,
        embeddings: np.ndarray,
        results: List[SearchResult],
    ) -> List[List[int]]:
        """Cluster search results using HDBSCAN algorithm."""
        import hdbscan

        n_results = len(results)
        if n_results < self.config.min_cluster_size:
            # ç»“æœå¤ªå°‘ï¼Œæ¯ä¸ªä½œä¸ºå•ç‹¬çš„èšç±»
            return [[i] for i in range(n_results)]

        # é…ç½® HDBSCAN
        clusterer = hdbscan.HDBSCAN(
            min_cluster_size=self.config.min_cluster_size,
            min_samples=self.config.min_samples,
            metric=self.config.metric,  # é€šå¸¸æ˜¯ 'cosine'
            cluster_selection_epsilon=self.config.cluster_selection_epsilon,
            allow_single_cluster=self.config.allow_single_cluster,
        )

        # æ‹Ÿåˆå¹¶è·å–èšç±»æ ‡ç­¾
        # Labels: -1 = noise, 0+ = cluster index
        labels = clusterer.fit_predict(embeddings)

        # æŒ‰æ ‡ç­¾åˆ†ç»„
        cluster_map: Dict[int, List[int]] = {}
        for idx, label in enumerate(labels):
            if label not in cluster_map:
                cluster_map[label] = []
            cluster_map[label].append(idx)

        # æ„å»ºç»“æœï¼šæ­£å¸¸èšç±»åœ¨å‰ï¼Œå™ªå£°ç‚¹ä½œä¸ºå•ç‹¬èšç±»åœ¨å
        clusters: List[List[int]] = []
        
        for label in sorted(cluster_map.keys()):
            if label >= 0:
                clusters.append(cluster_map[label])
        
        # å™ªå£°ç‚¹ä½œä¸ºå•ç‹¬èšç±»
        if -1 in cluster_map:
            for idx in cluster_map[-1]:
                clusters.append([idx])

        return clusters

    def select_representatives(
        self,
        clusters: List[List[int]],
        results: List[SearchResult],
    ) -> List[SearchResult]:
        """Select representative results from each cluster.
        
        é€‰æ‹©æ¯ä¸ªèšç±»ä¸­åˆ†æ•°æœ€é«˜çš„ç»“æœä½œä¸ºä»£è¡¨ã€‚
        """
        representatives: List[SearchResult] = []

        for cluster_indices in clusters:
            if not cluster_indices:
                continue

            # æ‰¾åˆ°èšç±»ä¸­åˆ†æ•°æœ€é«˜çš„ç»“æœ
            best_idx = max(cluster_indices, key=lambda i: results[i].score)
            representatives.append(results[best_idx])

        # æŒ‰åˆ†æ•°é™åºæ’åˆ—
        representatives.sort(key=lambda r: r.score, reverse=True)

        return representatives
```

---

## ç¬¬äº”å¹•ï¼šè¿›åŒ–çš„æ’æ§½ (The Upgrade)

### æ’æ§½ä¸€ï¼šè‡ªå®šä¹‰ Embedder

**åœºæ™¯**ï¼šéœ€è¦ä½¿ç”¨ç‰¹å®šçš„åµŒå…¥æ¨¡å‹

**æ–¹æ¡ˆ**ï¼šå®ç° `BaseEmbedder` æ¥å£

```python
from codexlens.semantic.base import BaseEmbedder

class CustomEmbedder(BaseEmbedder):
    def __init__(self, model_path: str):
        self.model = load_custom_model(model_path)
    
    @property
    def embedding_dim(self) -> int:
        return 512
    
    def embed(self, texts: List[str]) -> List[List[float]]:
        return [self.model.encode(t).tolist() for t in texts]
    
    def embed_single(self, text: str) -> List[float]:
        return self.model.encode(text).tolist()
```

### æ’æ§½äºŒï¼šæ–°å¢èšç±»ç­–ç•¥

**åœºæ™¯**ï¼šéœ€è¦ä¸åŒçš„èšç±»ç®—æ³•

**æ–¹æ¡ˆ**ï¼šå®ç° `BaseClusteringStrategy` æ¥å£

```python
from codexlens.search.clustering.base import BaseClusteringStrategy, ClusteringConfig

class AgglomerativeStrategy(BaseClusteringStrategy):
    """Agglomerative hierarchical clustering strategy."""
    
    def cluster(
        self,
        embeddings: np.ndarray,
        results: List[SearchResult],
    ) -> List[List[int]]:
        from sklearn.cluster import AgglomerativeClustering
        
        n_clusters = min(self.config.n_clusters, len(results))
        clusterer = AgglomerativeClustering(
            n_clusters=n_clusters,
            metric='cosine',
            linkage='average'
        )
        
        labels = clusterer.fit_predict(embeddings)
        
        # æŒ‰æ ‡ç­¾åˆ†ç»„...
        return clusters
```

### æ’æ§½ä¸‰ï¼šReranker äºŒé˜¶æ®µé‡æ’

**åœºæ™¯**ï¼šéœ€è¦æ›´é«˜ç²¾åº¦çš„æ’åº

**æ–¹æ¡ˆ**ï¼šä½¿ç”¨ Cross-Encoder è¿›è¡ŒäºŒé˜¶æ®µé‡æ’

```python
def cross_encoder_rerank(
    query: str,
    results: List[SearchResult],
    reranker: Any,
    top_k: int = 50,
) -> List[SearchResult]:
    """Second-stage reranking using a cross-encoder model."""
    if not results or reranker is None:
        return results

    rerank_count = min(top_k, len(results))
    pairs = [(query, r.excerpt or r.content) for r in results[:rerank_count]]

    # è·å–è·¨ç¼–ç å™¨åˆ†æ•°
    raw_scores = reranker.score_pairs(pairs)
    
    # å½’ä¸€åŒ–åˆ° [0, 1]
    min_s, max_s = min(raw_scores), max(raw_scores)
    if 0 <= min_s and max_s <= 1:
        probs = raw_scores
    else:
        probs = [sigmoid(s) for s in raw_scores]

    # ç»„åˆåˆ†æ•°ï¼š0.5 * RRF + 0.5 * Cross-Encoder
    for idx, result in enumerate(results[:rerank_count]):
        prev_score = result.score
        ce_prob = probs[idx]
        combined = 0.5 * prev_score + 0.5 * ce_prob
        
        results[idx] = SearchResult(
            path=result.path,
            score=combined,
            excerpt=result.excerpt,
            content=result.content,
            metadata={
                **result.metadata,
                "pre_cross_encoder_score": prev_score,
                "cross_encoder_prob": ce_prob,
            },
        )

    results.sort(key=lambda r: r.score, reverse=True)
    return results
```

---

## ğŸ”° ç ´æ¡ˆçº¿ç´¢æ¡£æ¡ˆ #14

> **æœ¬ç« å‘ç°**: CodexLens é€šè¿‡æ··åˆæœç´¢ï¼ˆSPLADE + Vector + RRFï¼‰å®ç°äº†é«˜å¬å›ç‡å’Œé«˜ç²¾ç¡®åº¦çš„å¹³è¡¡
> **å…³è”èµ„äº§**:
> - `codex-lens/src/codexlens/search/hybrid_search.py` â€” æ··åˆæœç´¢å¼•æ“
> - `codex-lens/src/codexlens/semantic/embedder.py` â€” åµŒå…¥æ¨¡å‹
> - `codex-lens/src/codexlens/semantic/splade_encoder.py` â€” SPLADE ç¼–ç å™¨
> - `codex-lens/src/codexlens/search/clustering/hdbscan_strategy.py` â€” HDBSCAN èšç±»
> - `codex-lens/src/codexlens/search/ranking.py` â€” RRF èåˆç®—æ³•
> **ä¸‹ä¸€ç« é¢„å‘Š**: LSP å›¾æ‰©å±• â€” å®æ—¶ä»£ç å…³ç³»çš„è¿½è¸ª...

**è°ƒæŸ¥è¿›åº¦**: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 75%

> **æ€è€ƒé¢˜**: å¦‚æœä½ è¦ä¸ºä¸€ä¸ªä»£ç åº“è®¾è®¡æœç´¢ç³»ç»Ÿï¼Œä½ ä¼šé€‰æ‹©ï¼š
> - çº¯ FTSï¼ˆä½å»¶è¿Ÿï¼Œä½å¬å›ï¼‰
> - çº¯å‘é‡ï¼ˆé«˜å¬å›ï¼Œé«˜å»¶è¿Ÿï¼‰
> - æ··åˆæœç´¢ï¼ˆå¹³è¡¡ï¼‰
> - æ··åˆ + é‡æ’ï¼ˆæœ€é«˜è´¨é‡ï¼Œæœ€é«˜å»¶è¿Ÿï¼‰
>
> ä½ çš„é€‰æ‹©ä¼šå¦‚ä½•å½±å“ç”¨æˆ·ä½“éªŒï¼Ÿ

---

## é™„å½•ï¼šäº‹æ•…å¤ç›˜æ¡£æ¡ˆ

### æ¡ˆä¾‹ #2ï¼šå‘é‡ç´¢å¼•æŸåå¯¼è‡´çš„æœç´¢å¤±è´¥

**æ—¶é—´çº¿**ï¼š
- 2024-04-20 14:30: ç”¨æˆ·æŠ¥å‘Š"æœç´¢è¿”å›ç©ºç»“æœ"
- 14:35: æ’æŸ¥å‘ç° `_vectors.hnsw` æ–‡ä»¶å¤§å°ä¸º 0
- 14:40: ç¡®è®¤æ˜¯ç£ç›˜ç©ºé—´ä¸è¶³å¯¼è‡´çš„å†™å…¥å¤±è´¥

**æ ¹å› åˆ†æ**ï¼š
```python
# é—®é¢˜ä»£ç ï¼šæ²¡æœ‰æ£€æŸ¥ç£ç›˜ç©ºé—´
def save_index(index: ANNIndex, path: Path):
    with open(path, 'wb') as f:
        pickle.dump(index, f)  # ç£ç›˜æ»¡æ—¶å†™å…¥ 0 å­—èŠ‚

# åç»­æœç´¢åŠ è½½ç©ºæ–‡ä»¶
def load_index(path: Path):
    with open(path, 'rb') as f:
        return pickle.load(f)  # EOFError
```

**ä¿®å¤æ–¹æ¡ˆ**ï¼š
```python
def load_index(path: Path) -> Optional[ANNIndex]:
    """Load ANN index with integrity check."""
    if not path.exists():
        return None
    
    # æ£€æŸ¥æ–‡ä»¶å¤§å°
    if path.stat().st_size == 0:
        logger.warning(f"Empty index file: {path}")
        return None
    
    # æ ¡éªŒé­”æ•°
    with open(path, 'rb') as f:
        magic = f.read(4)
        if magic != b'HNSW':
            logger.warning(f"Invalid index magic: {magic}")
            return None
    
    try:
        with open(path, 'rb') as f:
            return pickle.load(f)
    except Exception as e:
        logger.error(f"Failed to load index: {e}")
        return None
```

**æ•™è®­**ï¼š
1. æ–‡ä»¶æ“ä½œéœ€è¦å®Œæ•´æ€§æ£€æŸ¥
2. ç©ºæ–‡ä»¶æ˜¯ä¸€ç§å¸¸è§çš„æŸåå½¢å¼
3. æœç´¢ç³»ç»Ÿéœ€è¦ä¼˜é›…é™çº§ï¼ˆå›é€€åˆ° FTSï¼‰

---

## é™„å½•ï¼šå®Œæ•´ä»£ç å‚è€ƒ

### A. æŸ¥è¯¢æ„å›¾æ£€æµ‹

```python
# æ–‡ä»¶: codex-lens/src/codexlens/search/ranking.py

class QueryIntent(str, Enum):
    """Query intent for adaptive RRF weights."""
    KEYWORD = "keyword"    # ä»£ç æ„å›¾ï¼šå‡½æ•°åã€ç±»å
    SEMANTIC = "semantic"  # è¯­ä¹‰æ„å›¾ï¼šè‡ªç„¶è¯­è¨€æè¿°
    MIXED = "mixed"        # æ··åˆæ„å›¾

def detect_query_intent(query: str) -> QueryIntent:
    """Detect whether a query is code-like, natural-language, or mixed."""
    trimmed = (query or "").strip()
    if not trimmed:
        return QueryIntent.MIXED

    lower = trimmed.lower()
    word_count = len([w for w in re.split(r"\s+", trimmed) if w])

    # ä»£ç ä¿¡å·
    has_code_signals = bool(
        re.search(r"(::|->|\.)", trimmed)  # C++/Rust/JS è®¿é—®ç¬¦
        or re.search(r"[A-Z][a-z]+[A-Z]", trimmed)  # CamelCase
        or re.search(r"\b\w+_\w+\b", trimmed)  # snake_case
        or re.search(
            r"\b(def|class|function|const|let|var|import|from|return|async|await)\b",
            lower,
            flags=re.IGNORECASE,
        )
    )
    
    # è‡ªç„¶è¯­è¨€ä¿¡å·
    has_natural_signals = bool(
        word_count > 5
        or "?" in trimmed
        or re.search(r"\b(how|what|why|when|where)\b", trimmed, flags=re.IGNORECASE)
        or re.search(
            r"\b(handle|explain|fix|implement|create|build|use|find|search)\b",
            trimmed,
            flags=re.IGNORECASE,
        )
    )

    if has_code_signals and has_natural_signals:
        return QueryIntent.MIXED
    if has_code_signals:
        return QueryIntent.KEYWORD
    if has_natural_signals:
        return QueryIntent.SEMANTIC
    return QueryIntent.MIXED
```

### B. è‡ªé€‚åº”æƒé‡è°ƒæ•´

```python
def adjust_weights_by_intent(
    intent: QueryIntent,
    base_weights: Dict[str, float],
) -> Dict[str, float]:
    """Adjust RRF weights based on query intent."""
    use_splade = "splade" in base_weights
    
    if intent == QueryIntent.KEYWORD:
        # ä»£ç æ„å›¾ï¼šåå‘ç²¾ç¡®åŒ¹é…
        if use_splade:
            target = {"splade": 0.6, "vector": 0.4}
        else:
            target = {"exact": 0.5, "fuzzy": 0.1, "vector": 0.4}
    elif intent == QueryIntent.SEMANTIC:
        # è¯­ä¹‰æ„å›¾ï¼šåå‘å‘é‡æœç´¢
        if use_splade:
            target = {"splade": 0.3, "vector": 0.7}
        else:
            target = {"exact": 0.2, "fuzzy": 0.1, "vector": 0.7}
    else:
        target = dict(base_weights)
    
    # è¿‡æ»¤åˆ°æ´»è·ƒåç«¯
    keys = list(base_weights.keys())
    filtered = {k: float(target.get(k, 0.0)) for k in keys}
    return normalize_weights(filtered)
```

### C. ç¬¦å·æƒé‡æå‡

```python
def apply_symbol_boost(
    results: List[SearchResult],
    boost_factor: float = 1.5,
) -> List[SearchResult]:
    """Boost fused scores for results that include an explicit symbol match."""
    if not results or boost_factor <= 1.0:
        return results

    boosted_results: List[SearchResult] = []
    for result in results:
        has_symbol = bool(result.symbol_name)
        original_score = float(result.score)
        boosted_score = original_score * boost_factor if has_symbol else original_score

        metadata = {**result.metadata}
        if has_symbol:
            metadata["original_fusion_score"] = original_score
            metadata["boosted"] = True
            metadata["symbol_boost_factor"] = boost_factor

        boosted_results.append(
            SearchResult(
                path=result.path,
                score=boosted_score,
                excerpt=result.excerpt,
                content=result.content,
                metadata=metadata,
                symbol_name=result.symbol_name,
                symbol_kind=result.symbol_kind,
            )
        )

    boosted_results.sort(key=lambda r: r.score, reverse=True)
    return boosted_results
```

---

*ç‰ˆæœ¬: 1.0.0*
*ä¼šè¯: ANL-ccw-architecture-audit-2025-02-17*
*é£æ ¼: "è¯­ä¹‰è¿½è¸ªè€…"å™äº‹*
